{"cells":[{"cell_type":"markdown","id":"89bac08d-fe8f-4fe0-aa65-814e869d1c08","metadata":{"id":"89bac08d-fe8f-4fe0-aa65-814e869d1c08"},"source":["# Automatic Differentiation\n","\n","In this assignment, we will be implementing a class that supports [Automatic Differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation) on scalar values.\n","\n","Automatic Differentiation fundamentally relies on **chain rule** and **partial derivatives of composite functions** $f(x) = a(b(c(x))) \\implies f' = a' b' c'$ for accumulating gradients.\n","\n","As we saw in the lecture there are two approaches to implementing Automatic Differentation. **forward-mode/right-to-left** and **reverse-mode/left-to-right**.\n","\n","### Forward-mode\n","\n","We calculate the partial derivatives of functions in the order the functions are composed. For a function $f(x) = a(b(c(x)))$ this involves\n","1. Evaluate $c(x)$ and calculate $c' = \\frac{\\delta c}{\\delta x}$.\n","2. Evaluate $b(c(x))$. Calculate $b' = \\frac{\\delta b}{\\delta c}$ and use it to calculate $\\frac{\\delta b}{\\delta x} = b' c' = \\frac{\\delta b}{\\delta c} \\frac{\\delta c}{\\delta x}$. $c(x)$ and $c'$ are available from step 1.\n","3. Similarly evaluate $a(b(c(x)))$ and calculate $\\frac{\\delta b}{\\delta x}$. Use $b(c(x))$ and $b'c'$ from the previous step.\n","4. We now have $\\frac{\\delta f}{\\delta x}$.\n","\n","### Implementation\n","\n","We will do a minimal implementation that only supports a single scalar variable, you can refer to [micrograd](https://github.com/karpathy/micrograd/tree/master) to get an idea of backward-mode implementation with multi-variable support by storing the computation DAG.\n","\n","Complete the Python class \"ValueFwd\" to support forward-mode Automatic Differentiation as explained above.\n","1. The class should store two values, the value of the variable/constant and its gradient w.r.t the independent variable. (hint: the gradient should be initialized to 0 for constants and 1 for variables, we will only have a single variable).\n","2. **ValueFwd** should support basic arithmetic operations, similar to **torch.Tensor**. Complete the class methods to add this functionality to ValueFwd. \"\\_\\_add\\_\\_\" is already implemented to give you an idea.\n","3. **TODO** Complete the implementation of ValueFwd and run the test code. play around with the input to test function, you are also free to implement mode test cases."]},{"cell_type":"code","execution_count":26,"id":"28332d6a-857a-42a7-94aa-9c0181e1cfce","metadata":{"id":"28332d6a-857a-42a7-94aa-9c0181e1cfce","executionInfo":{"status":"ok","timestamp":1729196092258,"user_tz":240,"elapsed":184,"user":{"displayName":"Manisha Goyal","userId":"04083419833801613070"}}},"outputs":[],"source":["import numpy as np\n","from math import sin, cos\n","import math\n","from numpy import log\n","from typing import List, Tuple\n","from numpy.testing import assert_almost_equal"]},{"cell_type":"markdown","id":"2ca601b9-cb3e-459b-bc23-847c4b08b600","metadata":{"id":"2ca601b9-cb3e-459b-bc23-847c4b08b600"},"source":["# forward mode"]},{"cell_type":"code","execution_count":28,"id":"161f3fea-1724-4e00-bc91-64ddced37337","metadata":{"id":"161f3fea-1724-4e00-bc91-64ddced37337","executionInfo":{"status":"ok","timestamp":1729196097978,"user_tz":240,"elapsed":160,"user":{"displayName":"Manisha Goyal","userId":"04083419833801613070"}}},"outputs":[],"source":["class ValueFwd:\n","    \"\"\" stores a single scalar value and its gradient \"\"\"\n","    def __init__(self, data, diff = 0):\n","        self.v = data\n","        self.d = diff\n","\n","    def __add__(l, r):\n","        \"\"\" support for + on ValueFwd datatype.\n","\n","         l + r returns f, such that f.v = l.v + r.v and f.d = f' = df/dx.\n","\n","        Args:\n","            l (ValueFwd): ValueFwd to the left of +.\n","            r (ValueFwd): ValueFwd to the right of +.\n","\n","        Returns:\n","            ValueFwd with updated value and gradient.\n","        \"\"\"\n","        return ValueFwd(l.v + r.v, l.d + r.d)\n","\n","    def __sub__(l, r):\n","        \"\"\" support for - on ValueFwd datatype.\n","\n","         l - r returns f, such that f.v = l.v - r.v and f.d = f' = df/dx.\n","\n","        Args:\n","            l (ValueFwd): ValueFwd to the left of -.\n","            r (ValueFwd): ValueFwd to the right of -.\n","\n","        Returns:\n","            ValueFwd with updated value and gradient.\n","        \"\"\"\n","        return ValueFwd(l.v - r.v, l.d - r.d)\n","\n","    def __mul__(l, r):\n","        \"\"\" support for * on ValueFwd datatype.\n","\n","         l * r returns f, such that f.v = l.v * r.v and f.d = f' = df/dx.\n","\n","        Args:\n","            l (ValueFwd): ValueFwd to the left of *.\n","            r (ValueFwd): ValueFwd to the right of *.\n","\n","        Returns:\n","            ValueFwd with updated value and gradient.\n","        \"\"\"\n","        return ValueFwd(l.v * r.v, l.d * r.v + r.d * l.v)\n","\n","    def __truediv__(l, r):\n","        \"\"\" support for / on ValueFwd datatype.\n","\n","         l / r returns f, such that f.v = l.v / r.v and f.d = f' = df/dx.\n","\n","        Args:\n","            l (ValueFwd): ValueFwd to the left of /.\n","            r (ValueFwd): ValueFwd to the right of /.\n","\n","        Returns:\n","            ValueFwd with updated value and gradient.\n","        \"\"\"\n","        if r.v == 0:\n","            raise ZeroDivisionError(\"Division by zero\")\n","        return ValueFwd(l.v / r.v, (l.d * r.v - r.d * l.v) / (r.v ** 2))\n","\n","    def sin(self):\n","        \"\"\" support for self.sin() on ValueFwd datatype.\n","\n","        Args:\n","            self (ValueFwd): ValueFwd.\n","\n","        Returns:\n","            returns ValueFwd f, such that f.v = sin(self.v) and f.d = f' = df/dx\n","        \"\"\"\n","        return ValueFwd(sin(self.v), cos(self.v) * self.d)\n","\n","    def cos(self):\n","        \"\"\" support for self.cos() on ValueFwd datatype.\n","\n","        Args:\n","            self (ValueFwd): ValueFwd.\n","\n","        Returns:\n","            returns ValueFwd f, such that f.v = cos(self.v) and f.d = f' = df/dx\n","        \"\"\"\n","        return ValueFwd(cos(self.v), -sin(self.v) * self.d)\n","\n","    def exp(self, b):\n","        \"\"\" support for b^self on ValueFwd datatype.\n","\n","            self.exp(math.e) = e^self\n","\n","        Args:\n","            self (ValueFwd): ValueFwd.\n","            b (Number): A numerical value.\n","\n","        Returns:\n","            returns ValueFwd f, such that f.v = b^(self.v) and f.d = f' = df/dx\n","        \"\"\"\n","        return ValueFwd(b ** self.v, (b ** self.v) * log(b) * self.d)\n","\n","    def pow(self, p):\n","        \"\"\" support for pow on ValueFwd datatype.\n","\n","            self.pow(3) = self^3\n","\n","        Args:\n","            self (ValueFwd): ValueFwd.\n","            p (Number): A numerical value.\n","\n","        Returns:\n","            returns ValueFwd f, such that f.v = (self.v)^p and f.d = f' = df/dx\n","        \"\"\"\n","        return ValueFwd(self.v ** p, p * (self.v ** (p - 1)) * self.d)\n","\n","    def __repr__(self):\n","        return f\"v: {self.v}, d:{self.d}\""]},{"cell_type":"markdown","id":"dd7594c5-91b7-4cdb-90f2-912d0c5da30d","metadata":{"id":"dd7594c5-91b7-4cdb-90f2-912d0c5da30d"},"source":["# Test forward-mode automatic differentiation\n","\n","Test function 1 is: `test_f1(x)`\n","$$f(x) = \\sin\\left( \\frac{\\sqrt{e^x + 2}}{2}\\right)$$\n","\n","The gradient for Test 1 is: `test_df1`\n","$$\\frac{e^x \\cos\\left( \\frac{\\sqrt{e^x + 2}}{2} \\right)}{4 \\sqrt{e^x + 2}}$$\n"]},{"cell_type":"code","execution_count":29,"id":"55291991-2c18-47de-a91f-b330ede140bb","metadata":{"id":"55291991-2c18-47de-a91f-b330ede140bb","executionInfo":{"status":"ok","timestamp":1729196101008,"user_tz":240,"elapsed":200,"user":{"displayName":"Manisha Goyal","userId":"04083419833801613070"}}},"outputs":[],"source":["def test_f1(x):\n","    return ((x.exp(math.e) + ValueFwd(2)).pow(0.5)/ValueFwd(2)).sin()\n","\n","def test_df1(x):\n","    ex = math.exp(x)\n","    num = ex*cos(math.sqrt(ex+2)/2)\n","    den = 4 * math.sqrt(ex+2)\n","    return num/den"]},{"cell_type":"markdown","id":"4ca95c0d-aba3-4cee-b033-2acfe8e8c52d","metadata":{"id":"4ca95c0d-aba3-4cee-b033-2acfe8e8c52d"},"source":["## test Froward-mode Automatic Differentiation"]},{"cell_type":"code","execution_count":30,"id":"da485978-28e2-489f-9886-c7d0f06d1704","metadata":{"id":"da485978-28e2-489f-9886-c7d0f06d1704","executionInfo":{"status":"ok","timestamp":1729196101944,"user_tz":240,"elapsed":379,"user":{"displayName":"Manisha Goyal","userId":"04083419833801613070"}}},"outputs":[],"source":["assert_almost_equal(test_f1(ValueFwd(1, 1)).d, test_df1(1))"]},{"cell_type":"markdown","id":"abc2f1aa-8220-4c9e-b72a-c1abd715067d","metadata":{"id":"abc2f1aa-8220-4c9e-b72a-c1abd715067d"},"source":["# reverse mode"]},{"cell_type":"markdown","id":"9a10f19b-b72b-4b3d-95f6-5f95c76cc693","metadata":{"id":"9a10f19b-b72b-4b3d-95f6-5f95c76cc693"},"source":["### Reverse-mode\n","\n","We do two passes in the reverse mode of Automatic Differentiation. One forward and one backward.\n","\n","For a function $f(x) = a(b(c(x)))$. We first calculate the local partial derivatives e.g. $\\frac{\\delta c}{\\delta x}, \\frac{\\delta b}{\\delta c}, \\frac{\\delta a}{\\delta b}$ during the forward pass. and accumulate them in the reverse pass to get $\\frac{\\delta f}{\\delta x}$\n","1. Evaluate $c(x)$ and calculate $c' = \\frac{\\delta c}{\\delta x}$. Store the local gradient $c'$ for future use during the backward pass.\n","2. Evaluate $b(c(x))$. Calculate $b' = \\frac{\\delta b}{\\delta c}$ and store it for future use during the backward pass.\n","3. Similarly evaluate $a(b(c(x)))$ and calculate $\\frac{\\delta a}{\\delta b}$ and store it.\n","4. Initiate the backward pass to accumulate the local gradients and calculate $\\frac{\\delta f}{\\delta x}$\n","\n","### Implementation\n","\n","Reverse mode is a little more complicated than the forward mode. Here gradients are accumulated during the backward pass. so, we also have to store the computation DAG(Directed Acyclic Graph) so that the backward pass can happen in the correct order.\n","\n","Complete the Python class \"ValueBwd\" to support Reverse-mode Automatic Differentiation as explained above.\n","1. Similar to forward mode store the value of the variable/constant and its gradient w.r.t the independent variable(not updated in the forward pass). Since gradients are accumulated in the backward pass from output-to-input / left-to-right, we have to store the local gradients as well as dependencies i.e., what nodes does the current node depend on? Use `self._prev` list to store the **(input node, and local gradient)** tuples for the current node.\n","2. During the backward pass `backward` recursively propagate the gradients backward and populate `self.d` for the nodes."]},{"cell_type":"code","execution_count":31,"id":"b1c44fb3-b00f-4867-af43-595e8a4ab3ab","metadata":{"id":"b1c44fb3-b00f-4867-af43-595e8a4ab3ab","executionInfo":{"status":"ok","timestamp":1729196105870,"user_tz":240,"elapsed":183,"user":{"displayName":"Manisha Goyal","userId":"04083419833801613070"}}},"outputs":[],"source":["class ValueBwd:\n","    \"\"\" stores a single scalar value and its gradient, updates gradient through back-prop \"\"\"\n","    def __init__(self, data, prev = []):\n","        \"\"\" Value datatype that supports backpropagation on a single scalar variable.\n","\n","        Args:\n","            data (Number): value of the variable/constant.\n","            prev (List[Tuple[ValueBwd, Number]]): List of node(ValueBwd), local gradient tuples that the current node depends on.\n","                Empty list if the computational graph contains a single node.\n","        Returns:\n","            ValueBwd.\n","        \"\"\"\n","        self.v = data\n","        self.d = 0\n","        self._prev = prev\n","\n","    def __add__(l, r):\n","        \"\"\" support for + on ValueBwd datatype.\n","\n","        Args:\n","            l (ValueBwd): ValueBwd to the left of +.\n","            r (ValueBwd): ValueBwd to the right of +.\n","\n","        Returns:\n","            ValueBwd with updated value and gradient.\n","        \"\"\"\n","        return ValueBwd(l.v + r.v, [(l, 1), (r, 1)])\n","\n","    def __sub__(l, r):\n","        \"\"\" support for - on ValueBwd datatype.\n","\n","        Args:\n","            l (ValueBwd): ValueBwd to the left of -.\n","            r (ValueBwd): ValueBwd to the right of -.\n","\n","        Returns:\n","            ValueBwd: Current node of the computational graph.\n","        \"\"\"\n","        return ValueBwd(l.v - r.v, [(l, 1), (r, -1)])\n","\n","    def __mul__(l, r):\n","        \"\"\" support for * on ValueBwd datatype.\n","\n","        Args:\n","            l (ValueBwd): ValueBwd to the left of -.\n","            r (ValueBwd): ValueBwd to the right of -.\n","\n","        Returns:\n","            ValueBwd: Current node of the computational graph.\n","        \"\"\"\n","        return ValueBwd(l.v * r.v, [(l, r.v), (r, l.v)])\n","\n","    def __truediv__(l, r):\n","        \"\"\" support for / on ValueBwd datatype.\n","\n","        Args:\n","            l (ValueBwd): ValueBwd to the left of -.\n","            r (ValueBwd): ValueBwd to the right of -.\n","\n","        Returns:\n","            ValueBwd: Current node of the computational graph.\n","        \"\"\"\n","        if r.v == 0:\n","            raise ZeroDivisionError(\"Division by zero\")\n","        return ValueBwd(l.v / r.v, [(l, 1 / r.v), (r, -l.v / (r.v ** 2))])\n","\n","    def sin(self):\n","        \"\"\" support for sin(self) on ValueBwd datatype.\n","\n","        Args:\n","\n","        Returns:\n","            ValueBwd: Current node of the computational graph.\n","        \"\"\"\n","        return ValueBwd(sin(self.v), [(self, cos(self.v))])\n","\n","    def cos(self):\n","        \"\"\" support for cos(self) on ValueBwd datatype.\n","\n","        Returns:\n","            ValueBwd: Current node of the computational graph.\n","        \"\"\"\n","        return ValueBwd(cos(self.v), [(self, -sin(self.v))])\n","\n","    def exp(self, b):\n","        \"\"\" support for b^self on ValueBwd datatype.\n","\n","        Args:\n","            b (Number):  A numerical value.\n","\n","        Returns:\n","            ValueBwd: Current node of the computational graph.\n","        \"\"\"\n","        return ValueBwd(b ** self.v, [(self, (b ** self.v) * log(b))])\n","\n","    def pow(self, p):\n","        \"\"\" support for self^p on ValueBwd datatype.\n","\n","        Args:\n","            p (ValueBwd):  A numerical value.\n","\n","        Returns:\n","            ValueBwd: Current node of the computational graph.\n","        \"\"\"\n","        return ValueBwd(self.v ** p, [(self, p * (self.v ** (p - 1)))])\n","\n","    def backward(self, d):\n","        \"\"\" Updates local gradient and iteratively calls backward on self._prev.\n","\n","        Args:\n","            d (Number): Gradient Value.\n","\n","        Returns:\n","            None\n","        \"\"\"\n","        # Add the incoming gradient to the local gradient\n","        self.d += d\n","\n","        # Recursively propagate the gradients to previous nodes\n","        for prev_node, local_grad in self._prev:\n","            prev_node.backward(d * local_grad)\n","\n","    def __repr__(self):\n","        return f\"v: {self.v}, d:{self.d}\""]},{"cell_type":"markdown","id":"86c141b6-378a-42d8-9605-4e0f14396ec4","metadata":{"id":"86c141b6-378a-42d8-9605-4e0f14396ec4"},"source":["## Test reverse mode AD"]},{"cell_type":"code","execution_count":32,"id":"d366ff68-d0a7-45c7-9f7b-db2cdccdf115","metadata":{"id":"d366ff68-d0a7-45c7-9f7b-db2cdccdf115","executionInfo":{"status":"ok","timestamp":1729196109032,"user_tz":240,"elapsed":197,"user":{"displayName":"Manisha Goyal","userId":"04083419833801613070"}}},"outputs":[],"source":["def test_f1(x):\n","    return ((x.exp(math.e) + ValueBwd(2)).pow(0.5)/ValueBwd(2)).sin()\n","\n","def test_df1(x):\n","    ex = math.exp(x)\n","    num = ex*cos(math.sqrt(ex+2)/2)\n","    den = 4 * math.sqrt(ex+2)\n","    return num/den"]},{"cell_type":"code","execution_count":33,"id":"3cd93c6e-6f88-43ab-a902-be372b33f2f5","metadata":{"id":"3cd93c6e-6f88-43ab-a902-be372b33f2f5","executionInfo":{"status":"ok","timestamp":1729196110248,"user_tz":240,"elapsed":247,"user":{"displayName":"Manisha Goyal","userId":"04083419833801613070"}}},"outputs":[],"source":["x = 2\n","inp = ValueBwd(x)\n","o = test_f1(inp)\n","o.backward(1)"]},{"cell_type":"code","execution_count":34,"id":"a1d4033a-1d9a-4f2a-8ed8-916ff9666448","metadata":{"id":"a1d4033a-1d9a-4f2a-8ed8-916ff9666448","executionInfo":{"status":"ok","timestamp":1729196111294,"user_tz":240,"elapsed":327,"user":{"displayName":"Manisha Goyal","userId":"04083419833801613070"}}},"outputs":[],"source":["assert_almost_equal(inp.d, test_df1(2))"]},{"cell_type":"code","execution_count":null,"id":"c0d7447b-5bce-4c8c-a8da-839d56692701","metadata":{"id":"c0d7447b-5bce-4c8c-a8da-839d56692701"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"}},"nbformat":4,"nbformat_minor":5}